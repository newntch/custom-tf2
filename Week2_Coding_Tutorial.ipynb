{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Coding Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "slDi0LNSLwlk",
        "1tPbAdRYLwmH",
        "YLOK8GMOLwmP",
        "KA-VnvRpLwmW",
        "-djfT1vDLwma",
        "wxB-r6_sLwmd",
        "eASEXb7uLwml",
        "CndbEe9-Lwmw",
        "2PTkNo-uLwm-",
        "tyi9F1cRLwnF",
        "KUfxD0WHLwnU",
        "8lDJvPvfLwnd",
        "IuICkLdgLwnv",
        "EdlrTTxkLwn1",
        "LoxWasQKLwn5",
        "zS5yD7TaLwn7",
        "vqqV3Ox9LwoA",
        "5d8X74-yLwoL",
        "agL_w4bhLwoU",
        "EgdUwwomLwom",
        "FJWNhXxMLwov",
        "WDGZ4VF-LwpC",
        "PDCxC62VLwpF",
        "B0IxwSLpLwpN",
        "8Ljy2Ro9LwpS",
        "5z_etyCZLwpZ",
        "H7rFrXSkLwpf",
        "waZerBcdLwpj",
        "6euItJ8bLwpn"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newntch/custom-tf2/blob/main/Week2_Coding_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JsB_bKALwj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe2c565-0de3-4925-8193-fe36bb240ece"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT-hyAgFLwj7"
      },
      "source": [
        "# Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3ueZW9CLwj8"
      },
      "source": [
        " ## Coding tutorials\n",
        " #### [1. Keras datasets](#coding_tutorial_1)\n",
        " #### [2. Dataset generators](#coding_tutorial_2)\n",
        " #### [3. Keras image data augmentation](#coding_tutorial_3)\n",
        " #### [4. The Dataset class](#coding_tutorial_4)\n",
        " #### [5. Training with Datasets](#coding_tutorial_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF-0r_trLwj9"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## Keras datasets\n",
        "\n",
        "For a list of Keras datasets and documentation on recommended usage, see [this link](https://keras.io/datasets/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHF_JbgGLwj-"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyE5HuD8LwkB"
      },
      "source": [
        "#### Load the CIFAR-100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Flu2zSLwkC"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SQUmYRQdLwkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "179b3151-cc38-4146-b4de-a72cf55cd3b9"
      },
      "source": [
        "# Load the CIFAR-100 dataset\n",
        "(train_images, train_label), (test_images, test_labels) = cifar100.load_data(label_mode='fine')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l75ozQVFLwkI"
      },
      "source": [
        "# Confirm that reloading the dataset does not require a download\n",
        "\n",
        "(train_images, train_label), (test_images, test_labels) = cifar100.load_data(label_mode='fine')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1UEPzrYLwkL"
      },
      "source": [
        "#### Examine the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDYN-cq9Mc2Q"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The additional files required for this tutorial can be downloaded from the following link:\n",
        "\n",
        "cifar100_fine_labels: https://drive.google.com/open?id=1WFW1cj8v_5z1pGvq6htQyFUPrJP-Z2v5\n",
        "\n",
        "cifar100_coarse_labels: https://drive.google.com/open?id=1Jmt7o-6sP85D7iRORk5tJqJMN3wCP12p\n",
        "\n",
        "You should store these files in Drive for use in this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiplZnP2ND6o"
      },
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpkIbhLTLwkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb810c1-860f-4d1d-8e83-6eb9de373de0"
      },
      "source": [
        "# Examine the shape of the data.\n",
        "\n",
        "print(train_images.shape)\n",
        "print(train_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY58jmf5LwkS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "30a57718-b13d-4240-fbcf-df5bcb63c7ea"
      },
      "source": [
        "# Examine one of the images and its corresponding label\n",
        "plt.imshow(train_images[500])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7ddd8e4320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bojn7VsfLwkV"
      },
      "source": [
        "# Load the list of labels from a JSON file\n",
        "\n",
        "import json\n",
        "\n",
        "with open('path/to/cifar100_fine_labels.json', 'r') as fine_labels:\n",
        "    cifar100_fine_labels = json.load(fine_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcULe51BLwkZ"
      },
      "source": [
        "The list of labels for the CIFAR-100 dataset are available [here](https://www.cs.toronto.edu/~kriz/cifar.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXq95zzcLwka"
      },
      "source": [
        "# Print a few of the labels\n",
        "cifar100_fine_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6Oqo7xKLwkd"
      },
      "source": [
        "# Print the corresponding label for the example above\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKQA_qggLwkh"
      },
      "source": [
        "#### Load the data using different label modes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkLZ3caeLwki"
      },
      "source": [
        "# Display a few examples from category 87 (index 86) and the list of labels\n",
        "\n",
        "examples = train_images[(train_labels.T == 86)[0]][:3]\n",
        "fig, ax = plt.subplots(1,3)\n",
        "ax[0].imshow(examples[0])\n",
        "ax[1].imshow(examples[1])\n",
        "ax[2].imshow(examples[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdP2x7rFLwkk"
      },
      "source": [
        "# Reload the data using the 'coarse' label mode\n",
        "(train_images, train_label), (test_images, test_labels) = cifar100.load_data(label_mode='coarse')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVizCMTSLwkn"
      },
      "source": [
        "# Display three images from the dataset with the label 6 (index 5)\n",
        "\n",
        "examples = train_images[(train_labels.T == 5)[0]][:3]\n",
        "fig, ax = plt.subplots(1,3)\n",
        "ax[0].imshow(examples[0])\n",
        "ax[1].imshow(examples[1])\n",
        "ax[2].imshow(examples[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_MZIWCFLwks"
      },
      "source": [
        "# Load the list of coarse labels from a JSON file\n",
        "\n",
        "with open('path/to/cifar100_coarse_labels.json', 'r') as coarse_labels:\n",
        "    cifar100_coarse_labels = json.load(coarse_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wbvYhD48Lwku"
      },
      "source": [
        "# Print a few of the labels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KGMaEInLwkx"
      },
      "source": [
        "# Print the corresponding label for the example above\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZDJui8OLwk1"
      },
      "source": [
        "#### Load the IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKylrcBKLwk2"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWNp9BGZLwk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd57913-efc3-4174-9595-78a6b09c7b20"
      },
      "source": [
        "# Load the IMDB dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTHEOfQ1Lwk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106cb23f-02ba-408d-92dc-86feddd7d2b7"
      },
      "source": [
        "# Print an example from the training dataset, along with its corresponding label\n",
        "print(train_data[0])\n",
        "print(train_labels[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UikjKYS5Lwk-"
      },
      "source": [
        "# Get the lengths of the input sequences\n",
        "\n",
        "sequence_lengths = [len(seq) for seq in train_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QbK1eZOLwlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdcd2b90-3eee-4d06-961e-ad110a89f5fd"
      },
      "source": [
        "# Determine the maximum and minimum sequence length\n",
        "print(np.max(sequence_lengths))\n",
        "print(np.min(sequence_lengths))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2494\n",
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfxdEsETLwlD"
      },
      "source": [
        "#### Using Keyword Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_rJ9cHDLwlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a7fc7d-4615-48de-ea9a-ad19ec736fae"
      },
      "source": [
        "# Load the data ignoring the 50 most frequent words, use oov_char=2 (this is the default)\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(skip_top=50, oov_char=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfrBgZWXLwlH"
      },
      "source": [
        "# Get the lengths of the input sequences\n",
        "\n",
        "sequence_lengths = [len(seq) for seq in train_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqSdCKqxLwlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0536ee0e-83cc-481d-c897-a034e8057ed0"
      },
      "source": [
        "# Determine the maximum and minimum sequence length\n",
        "print(np.max(sequence_lengths))\n",
        "print(np.min(sequence_lengths))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2494\n",
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9RjMJaJLwlL"
      },
      "source": [
        "# Define functions for filtering the sequences\n",
        "\n",
        "def remove_oov_char(element):\n",
        "    ''' Filter function for removing the oov_char. '''\n",
        "    return [word for word in element if word!=2]\n",
        "\n",
        "def filter_list(lst):\n",
        "    ''' Run remove_oov_char on elements in a list. '''\n",
        "    return [remove_oov_char(element) for element in lst]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eBoicx1LwlO"
      },
      "source": [
        "# Remove the oov_char from the sequences using the filter_list function\n",
        "train_data = filter_list(train_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ0myFkyLwlR"
      },
      "source": [
        "# Get the lengths of the input sequences\n",
        "\n",
        "sequence_lengths = [len(seq) for seq in train_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy5jFTr3LwlU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b8587b-9671-40f8-97a5-9780a4ece2b1"
      },
      "source": [
        "# Determine the maximum and minimum sequence length\n",
        "print(np.max(sequence_lengths))\n",
        "print(np.min(sequence_lengths))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1648\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43w36xfNLwlW"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Dataset generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GVjAo5nLwlX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeksQNB9LwlZ"
      },
      "source": [
        "#### Load the UCI Fertility Dataset\n",
        "\n",
        "We will be using a dataset available at https://archive.ics.uci.edu/ml/datasets/Fertility from UC Irvine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDMv0Yc6NGd9"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset required for this tutorial can be downloaded from the following link:\n",
        "\n",
        "https://drive.google.com/open?id=1OA0lwa5YLDs1njS377jbqPpMSlH5TzQV\n",
        "\n",
        "You should store this file in Drive for use in this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK-HCBxlNGpb"
      },
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKfmk3zpLwla"
      },
      "source": [
        "# Load the fertility dataset\n",
        "\n",
        "headers = ['Season', 'Age', 'Diseases', 'Trauma', 'Surgery', 'Fever', 'Alcohol', 'Smoking', 'Sitting', 'Output']\n",
        "fertility = pd.read_csv('path/to/fertility_diagnosis.txt', delimiter=',', header=None, names=headers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWOyjhCJLwld"
      },
      "source": [
        "# Print the shape of the DataFrame\n",
        "\n",
        "print(fertility.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwGWKnbbLwlf"
      },
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slDi0LNSLwlk"
      },
      "source": [
        "#### Process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCmzD-5HLwll"
      },
      "source": [
        "# Map the 'Output' feature from 'N' to 0 and from 'O' to 1\n",
        "\n",
        "fertility['Output'] = fertility['Output'].map(lambda x : 0.0 if x=='N' else 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yjVX1KMLwlo"
      },
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ5qjO5ALwlq"
      },
      "source": [
        "# Convert the DataFrame so that the features are mapped to floats\n",
        "\n",
        "fertility = fertility.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWIZ-en6Lwls"
      },
      "source": [
        "# Shuffle the DataFrame\n",
        "\n",
        "fertility = fertility.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4HH_ObWqLwlu"
      },
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oREiQC5wLwly"
      },
      "source": [
        "# Convert the field Season to a one-hot encoded vector\n",
        "\n",
        "fertility = pd.get_dummies(fertility, prefix='Season', columns=['Season'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDCUdq3VLwl2"
      },
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eukmHFMILwl5"
      },
      "source": [
        "# Move the Output column such that it is the last column in the DataFrame\n",
        "\n",
        "fertility.columns = [col for col in fertility.columns if col != 'Output'] + ['Output']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-anaeBLLwmB"
      },
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEyYKSueLwmD"
      },
      "source": [
        "# Convert the DataFrame to a numpy array.\n",
        "\n",
        "fertility = fertility.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tPbAdRYLwmH"
      },
      "source": [
        "#### Split the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1qYU7lcLwmI"
      },
      "source": [
        "# Split the dataset into training and validation set\n",
        "\n",
        "training = fertility[0:70]\n",
        "validation = fertility[70:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnwFqRrLLwmL"
      },
      "source": [
        "# Verify the shape of the training data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvCdYLU5LwmN"
      },
      "source": [
        "# Separate the features and labels for the validation and training data\n",
        "\n",
        "training_features = training[:,0:-1]\n",
        "training_labels = training[:,-1]\n",
        "validation_features = validation[:,0:-1]\n",
        "validation_labels = validation[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLOK8GMOLwmP"
      },
      "source": [
        "#### Create the Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uINAoFn5LwmQ"
      },
      "source": [
        "# Create a function that returns a generator producing inputs and labels\n",
        "\n",
        "def get_generator(features, labels, batch_size=1):\n",
        "    for n in range(int(len(features)/batch_size)):\n",
        "        yield (features[n*batch_size: (n+1)*batch_size], labels[n*batch_size: (n+1)*batch_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_zKx3oJLwmS"
      },
      "source": [
        "# Apply the function to our training features and labels with a batch size of 10\n",
        "\n",
        "train_generator = get_generator(training_features, training_labels, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y3jCTRuLwmT"
      },
      "source": [
        "# Test the generator using the next() function\n",
        "\n",
        "next(train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA-VnvRpLwmW"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5T3dXGZLwmW"
      },
      "source": [
        "# Create a model using Keras with 3 layers\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Input, BatchNormalization\n",
        "\n",
        "input_shape = (12,)\n",
        "output_shape = (1,)\n",
        "\n",
        "model_input = Input(input_shape)\n",
        "batch_1 = BatchNormalization(momentum=0.8)(model_input)\n",
        "dense_1 = Dense(100, activation='relu')(batch_1)\n",
        "batch_2 = BatchNormalization(momentum=0.8)(dense_1)\n",
        "output = Dense(1, activation='sigmoid')(batch_2)\n",
        "\n",
        "model = Model([model_input], output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCBoGMMULwmY"
      },
      "source": [
        "# Display the model summary to show the resultant structure\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-djfT1vDLwma"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_PyDixLLwma"
      },
      "source": [
        "# Create the optimizer object\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZUjV3LpLwmc"
      },
      "source": [
        "# Compile the model with loss function and metric\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxB-r6_sLwmd"
      },
      "source": [
        "#### Train and evaluate the model using the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCmNFC0eLwmd"
      },
      "source": [
        "# Calculate the number of training steps per epoch for the given batch size.\n",
        "\n",
        "batch_size = 5\n",
        "train_steps = len(training) // batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCyaX46bLwmf"
      },
      "source": [
        "# Set the epochs to 3\n",
        "\n",
        "epochs = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TKc7muJLwmg"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_generator = get_generator(train_features, training_labels, batch_size=batch_size)\n",
        "  validation_generator = get_generator(validation_Features, validation_labels, batch_size=30)\n",
        "  model.fit_generator(train_generator, steps_per_epoch=train_steps, \n",
        "                      validation_data=validation_generator, validation_steps=1)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5-6hh7BsLwmj"
      },
      "source": [
        "# Try to run the fit_generator function once more; observe what happens\n",
        "\n",
        "model.fit_generator(train_generator, steps_per_epoch=train_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eASEXb7uLwml"
      },
      "source": [
        "#### Make an infinitely looping generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX2TZzl2Lwmm"
      },
      "source": [
        "# Create a function that returns an infinitely looping generator\n",
        "\n",
        "def get_generator_cyclic(features, labels, batch_size=1):\n",
        "  while True:\n",
        "  for n in range(int(len(features)/batch_size)):\n",
        "    yield (features[n*batch_size: (n+1)*batch_size], labels[n*batch_size: (n+1)*batch_size])\n",
        "  permuted = np.random.permutation(len(features))\n",
        "  features = features[permuted]\n",
        "  labels = labels[permuted]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkZIx02TLwmo"
      },
      "source": [
        "# Create a generator using this function.\n",
        "\n",
        "train_generator_cyclic = get_generator_cyclic(training_features, training_labels, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCXB7YBZLwmr"
      },
      "source": [
        "# Assert that the new cyclic generator does not raise a StopIteration\n",
        "\n",
        "for i in range(2*train_steps):\n",
        "    next(train_generator_cyclic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJXeBNFALwms"
      },
      "source": [
        "# Generate a cyclic validation generator\n",
        "\n",
        "validation_generator_cyclic = get_generator_cyclic(validation_features, validation_labels, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-ezRy60iLwmu"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(training_generator_cyclic, steps_per_epoch=train_steps, \n",
        "          validation_data=validation_generator_cyclic, validation_steps=1, epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CndbEe9-Lwmw"
      },
      "source": [
        "#### Evaluate the model and get predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAPuIgstLwmw"
      },
      "source": [
        "# Let's obtain a validation data generator.\n",
        "\n",
        "validation_generator = get_generator(validation_features, validation_labels, batch_size=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs38y9RaLwmz"
      },
      "source": [
        "# Get predictions on the validation data\n",
        "\n",
        "prediction = model.predict_generator(validation_generator, steps=1)\n",
        "print(np.round(prediciton.T[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4Ce17XxwLwm4"
      },
      "source": [
        "# Print the corresponding validation labels\n",
        "\n",
        "print(validation_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUUeWVS1Lwm6"
      },
      "source": [
        "# Obtain a validation data generator\n",
        "\n",
        "validation_generator = get_generator(validation_features, validation_labels, batch_size=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shBJD8w4Lwm7"
      },
      "source": [
        "# Evaluate the model\n",
        "\n",
        "model.evaluate(validation_generator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgEaDU0TLwm8"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## Keras image data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fea1o5zdlS0p"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset required for this tutorial can be downloaded from the following link:\n",
        "\n",
        "https://drive.google.com/open?id=11Y43ta5gT672L3sfJFR2DvPs-ralY5Pd\n",
        "\n",
        "You should store these files in Drive for use in this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPDIjLB8lW6v"
      },
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdw6c9W-Lwm8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PTkNo-uLwm-"
      },
      "source": [
        "#### Load the CIFAR-10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-174tvQULwm-"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_ipqjVnLwnA"
      },
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "\n",
        "(training_features, training_labels), (test_features, test_labels) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH-q30M2LwnC"
      },
      "source": [
        "# Convert the labels to a one-hot encoding\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "training_labels = tf.keras.utils.to_categorical(training_labels, num_classes)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyi9F1cRLwnF"
      },
      "source": [
        "#### Create a generator function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiTmYP9HLwnF"
      },
      "source": [
        "# Create a function that returns a data generator\n",
        "\n",
        "def get_generator(features, labels, batch_size=1):\n",
        "    for n in range(int(len(features)/batch_size)):\n",
        "        yield (features[n*batch_size:(n+1)*batch_size], labels[n*batch_size:(n+1)*batch_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXXF9JKNLwnI"
      },
      "source": [
        "# Use the function we created to get a training data generator with a batch size of 1\n",
        "\n",
        "training_generator = get_generator(training_features, training_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaflrvQYLwnL"
      },
      "source": [
        "# Assess the shape of the items generated by training_generator using the `next` function to yield an item.\n",
        "\n",
        "image, label = next(training_generator)\n",
        "print(image.shape)\n",
        "print(label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXS3YwfdLwnQ"
      },
      "source": [
        "# Test the training generator by obtaining an image using the `next` generator function, and then using imshow to plot it.\n",
        "# Print the corresponding label\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "image, label = next(training_generator)\n",
        "image_unbatched = image[0,:,:,:]\n",
        "imshow(image_unbatched)\n",
        "print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRHQU5fyLwnS"
      },
      "source": [
        "# Reset the generator by re-running the `get_generator` function.\n",
        "\n",
        "train_generator = get_generator(training_features, training_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUfxD0WHLwnU"
      },
      "source": [
        "#### Create a data augmention generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S209JxdNLwnU"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R80OSZxoLwnX"
      },
      "source": [
        "# Create a function to convert an image to monochrome\n",
        "\n",
        "def monochrome(x):\n",
        "    def func_bw(a):\n",
        "        average_colour = np.mean(a)\n",
        "        return [average_colour, average_colour, average_colour]\n",
        "    x = np.apply_along_axis(func_bw, -1, x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B_-LrD8LwnZ"
      },
      "source": [
        "# Create an ImageDataGenerator object\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wheasgOSLwna"
      },
      "source": [
        "Check [the documentation](https://keras.io/preprocessing/image/) for the full list of image data augmentation options. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElwkY6hzLwna"
      },
      "source": [
        "# Create an iterable generator using the `flow` function\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "G_MojVrRLwnc"
      },
      "source": [
        "# Show a sample from the generator and compare with the original\n",
        "\n",
        "image, label = next(image_generator_iterable)\n",
        "image_orig, label_orig = next(train_generator)\n",
        "figs, axes = plt.subplots(1,2)\n",
        "axes[0].imshow(image[0,:,:,:])\n",
        "axes[0].set_title('Transformed')\n",
        "axes[1].imshow(image_orig[0,:,:,:])\n",
        "axes[1].set_title('Original')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lDJvPvfLwnd"
      },
      "source": [
        "#### Flow from directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0sOke4dLwne"
      },
      "source": [
        "# Inspect the directory structure\n",
        "\n",
        "train_path = 'path/to/flowers-recognition-split/train'\n",
        "val_path = 'path/to/flowers-recognition-split/val'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU_0lGLnLwnf"
      },
      "source": [
        "# Create an ImageDataGenerator object\n",
        "\n",
        "datagenerator = ImageDataGenerator(rescale=(1/255.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNSTFnZpLwng"
      },
      "source": [
        "classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoSrRM2vLwni"
      },
      "source": [
        "# Create a training data generator\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4iABrtkLwnj"
      },
      "source": [
        "# Create a validation data generator\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xttbpzhiLwnn"
      },
      "source": [
        "# Get and display an image and label from the training generator\n",
        "\n",
        "x = next(train_generator)\n",
        "imshow(x[0][4])\n",
        "print(x[1][4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eubMyZu7Lwnu"
      },
      "source": [
        "# Reset the training generator\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuICkLdgLwnv"
      },
      "source": [
        "#### Create a model to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Vli6KboxLwnv"
      },
      "source": [
        "# Build a CNN model\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Input((16,16,3)))\n",
        "model.add(Conv2D(8, (8, 8), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((4,4)))\n",
        "model.add(Conv2D(8, (8, 8), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(4, (4, 4), padding='same', activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EpyPWDtLwnw"
      },
      "source": [
        "# Create an optimizer object\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkzxS4w8Lwny"
      },
      "source": [
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRFuAO0dLwnz"
      },
      "source": [
        "# Print the model summary\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdlrTTxkLwn1"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cXhw2VhLwn1"
      },
      "source": [
        "# Calculate the training generator and test generator steps per epoch\n",
        "\n",
        "train_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
        "val_steps = val_generator.n // val_generator.batch_size\n",
        "print(train_steps_per_epoch, val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3daJBkILwn4"
      },
      "source": [
        "# Fit the model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoxWasQKLwn5"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-85jqQXiLwn6"
      },
      "source": [
        "# Evaluate the model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS5yD7TaLwn7"
      },
      "source": [
        "#### Predict using the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz3CsLtgLwn8"
      },
      "source": [
        "# Predict labels with the model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW1KSTrfLwn_"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## The Dataset Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB1fIrr8lmUb"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset required for this tutorial can be downloaded from the following link:\n",
        "\n",
        "https://drive.google.com/open?id=1BAjGPFlpqsDdWof50Ng3Fmju5O8F1_uZ\n",
        "\n",
        "You should store these files in Drive for use in this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfnXkSB3lmek"
      },
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xONz3oFLwn_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqqV3Ox9LwoA"
      },
      "source": [
        "#### Create a simple dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7i5Nkm6LwoA"
      },
      "source": [
        "x = np.zeros((100,10,2,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRIbPGW0LwoC"
      },
      "source": [
        "# Create a dataset from the tensor x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPIBuRAPLwoE"
      },
      "source": [
        "# Inspect the Dataset object\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwXTdMGYLwoF"
      },
      "source": [
        "x2 = [np.zeros((10,2,2)), np.zeros((5,2,2))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u33v-mOtLwoG"
      },
      "source": [
        "# Try creating a dataset from the tensor x2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlwcxydiLwoH"
      },
      "source": [
        "x2 = [np.zeros((10,1)), np.zeros((10,1)), np.zeros((10,1))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWA_WwA9LwoI"
      },
      "source": [
        "# Create another dataset from the new x2 and inspect the Dataset object\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaAx8T-8LwoK"
      },
      "source": [
        "# Print the element_spec\n",
        "\n",
        "print(dataset2.element_spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d8X74-yLwoL"
      },
      "source": [
        "#### Create a zipped dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pll2VDVALwoM"
      },
      "source": [
        "# Combine the two datasets into one larger dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFMyovAFLwoO"
      },
      "source": [
        "# Print the element_spec\n",
        "\n",
        "print(dataset_zipped.element_spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKlzGVR-LwoQ"
      },
      "source": [
        "# Define a function to find the number of batches in a dataset\n",
        "\n",
        "def get_batches(dataset):\n",
        "    iter_dataset = iter(dataset)\n",
        "    i = 0\n",
        "    try:\n",
        "        while next(iter_dataset):\n",
        "            i = i+1\n",
        "    except:\n",
        "        return i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pN5h3VbLwoS"
      },
      "source": [
        "# Find the number of batches in the zipped Dataset\n",
        "\n",
        "get_batches(dataset_zipped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agL_w4bhLwoU"
      },
      "source": [
        "#### Create a dataset from numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8BG27uVLwoU"
      },
      "source": [
        "# Load the MNIST dataset\n",
        "\n",
        "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print(type(train_features), type(train_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9z_XKC5LwoX"
      },
      "source": [
        "# Create a Dataset from the MNIST data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2huRDyELwob"
      },
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "print(mnist_dataset.element_spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYy7jUxxLwog"
      },
      "source": [
        "# Inspect the length of an element using the take method\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmU3DT_0Lwok"
      },
      "source": [
        "# Examine the shapes of the data\n",
        "\n",
        "print(element[0].shape)\n",
        "print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgdUwwomLwom"
      },
      "source": [
        "#### Create a dataset from text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc2ox0fXLwon"
      },
      "source": [
        "# Print the list of text files\n",
        "\n",
        "text_files = sorted([f.path for f in os.scandir('path/to/shakespeare')])\n",
        "\n",
        "print(text_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K91kV97kLwoo"
      },
      "source": [
        "# Load the first file using python and print the first 5 lines.\n",
        "\n",
        "with open(text_files[0], 'r') as fil:\n",
        "    contents = [fil.readline() for i in range(5)]\n",
        "    for line in contents:\n",
        "        print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MMqVhxNLwop"
      },
      "source": [
        "# Load the lines from the files into a dataset using TextLineDataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEM2up5CLwoq"
      },
      "source": [
        "# Use the take method to get and print the first 5 lines of the dataset\n",
        "\n",
        "first_5_lines_dataset = iter(shakespeare_dataset.take(5))\n",
        "lines = [line for line in first_5_lines_dataset]\n",
        "for line in lines:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t06mXu1oLwor"
      },
      "source": [
        "# Compute the number of lines in the first file\n",
        "\n",
        "lines = []\n",
        "with open(text_files[0], 'r') as fil:\n",
        "    line = fil.readline()\n",
        "    while line:\n",
        "        lines.append(line)\n",
        "        line = fil.readline()\n",
        "    print(len(lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS3TysHqLwot"
      },
      "source": [
        "# Compute the number of lines in the shakespeare dataset we created\n",
        "\n",
        "shakespeare_dataset_iterator = iter(shakespeare_dataset)\n",
        "lines = [line for line in shakespeare_dataset_iterator]\n",
        "print(len(lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJWNhXxMLwov"
      },
      "source": [
        "#### Interleave lines from the text data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "n7ukqFrcLwov"
      },
      "source": [
        "# Create a dataset of the text file strings\n",
        "\n",
        "text_files_dataset = tf.data.Dataset.from_tensor_slices(text_files)\n",
        "files = [file for file in text_files_dataset]\n",
        "for file in files:\n",
        "    print(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGgRFvR_Lwox"
      },
      "source": [
        "# Interleave the lines from the text files\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x88YAMsuLwoz"
      },
      "source": [
        "# Print the first 10 elements of the interleaved dataset\n",
        "\n",
        "lines = [line for line in iter(interleaved_shakespeare_dataset.take(10))]\n",
        "for line in lines:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Px88RXmLwo0"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_5\"></a>\n",
        "## Training with Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfCscLFLLwo1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEq4s3oHLwo3"
      },
      "source": [
        "#### Load the UCI Bank Marketing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NukSa_ESmgRg"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset required for this tutorial can be downloaded from the following link:\n",
        "\n",
        "https://drive.google.com/open?id=1cNtP4iDyGhF620ZbmJdmJWYQrRgJTCum\n",
        "\n",
        "You should store these files in Drive for use in this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TD43V9imgeD"
      },
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mWVTUMGLwo4"
      },
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "\n",
        "bank_dataframe = pd.read_csv('path/to/bank/bank-full.csv', delimiter=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yhXvshyRLwo6"
      },
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "bank_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTEQWdkfLwo9"
      },
      "source": [
        "# Print the shape of the DataFrame\n",
        "\n",
        "print(bank_dataframe.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8ORhBIMLwo_"
      },
      "source": [
        "# Select features from the DataFrame\n",
        "\n",
        "features = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
        "            'loan', 'contact', 'campaign', 'pdays', 'poutcome']\n",
        "labels = ['y']\n",
        "\n",
        "bank_dataframe = bank_dataframe.filter(features + labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OvhQZha0LwpA"
      },
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "bank_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDGZ4VF-LwpC"
      },
      "source": [
        "#### Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5OJBhpHLwpC"
      },
      "source": [
        "# Convert the categorical features in the DataFrame to one-hot encodings\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "categorical_features = ['default', 'housing', 'job', 'loan', 'education', 'contact', 'poutcome']\n",
        "\n",
        "for feature in categorical_features:\n",
        "    bank_dataframe[feature] = tuple(encoder.fit_transform(bank_dataframe[feature]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_H29rfTRLwpD"
      },
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "bank_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzA8Yfl7LwpE"
      },
      "source": [
        "# Shuffle the DataFrame\n",
        "\n",
        "bank_dataframe = bank_dataframe.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDCxC62VLwpF"
      },
      "source": [
        "#### Create the Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58t232NWLwpF"
      },
      "source": [
        "# Convert the DataFrame to a Dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIam8gBSLwpH"
      },
      "source": [
        "# Inspect the Dataset object\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0IxwSLpLwpN"
      },
      "source": [
        "#### Filter the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0m_HNo6LwpO"
      },
      "source": [
        "# First check that there are records in the dataset for non-married individuals\n",
        "\n",
        "def check_divorced():\n",
        "    bank_dataset_iterable = iter(bank_dataset)\n",
        "    for x in bank_dataset_iterable:\n",
        "        if x['marital'] != 'divorced':\n",
        "            print('Found a person with marital status: {}'.format(x['marital']))\n",
        "            return\n",
        "    print('No non-divorced people were found!')\n",
        "\n",
        "check_divorced()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dpKPS8vLwpP"
      },
      "source": [
        "# Filter the Dataset to retain only entries with a 'divorced' marital status\n",
        "\n",
        "bank_dataset = bank_dataset.filter(lambda x : tf.equal(x['marital'], tf.constant([b'divorced']))[0] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDqnULCjLwpR"
      },
      "source": [
        "# Check the records in the dataset again\n",
        "\n",
        "check_divorced()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ljy2Ro9LwpS"
      },
      "source": [
        "#### Map a function over the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qakJuUiNLwpS"
      },
      "source": [
        "# Convert the label ('y') to an integer instead of 'yes' or 'no'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1YcLNq7LwpT"
      },
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "bank_dataset.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bSkBnzlLwpV"
      },
      "source": [
        "# Remove the 'marital' column\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lK3OmJ4LwpW"
      },
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "bank_dataset.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z_etyCZLwpZ"
      },
      "source": [
        "#### Create input and output data tuples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PStTjmKLwpZ"
      },
      "source": [
        "# Create an input and output tuple for the dataset\n",
        "\n",
        "def map_feature_label(x):\n",
        "    features = [[x['age']], [x['balance']], [x['campaign']], x['contact'], x['default'],\n",
        "                x['education'], x['housing'], x['job'], x['loan'], [x['pdays']], x['poutcome']]\n",
        "    return (tf.concat(features, axis=0), x['y'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pKehf3i-Lwpc"
      },
      "source": [
        "# Map this function over the dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yet0K8SjLwpd"
      },
      "source": [
        "# Inspect the Dataset object\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7rFrXSkLwpf"
      },
      "source": [
        "#### Split into a training and a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u16htz4qLwpg"
      },
      "source": [
        "# Determine the length of the Dataset\n",
        "\n",
        "dataset_length = 0\n",
        "for _ in bank_dataset:\n",
        "    dataset_length += 1\n",
        "print(dataset_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFo3xsyQLwph"
      },
      "source": [
        "# Make training and validation sets from the dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waZerBcdLwpj"
      },
      "source": [
        "#### Build a classification model\n",
        "\n",
        "Now let's build a model to classify the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMFNjG7PLwpj"
      },
      "source": [
        "# Build a classifier model\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Input, Concatenate, BatchNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(30,)))\n",
        "model.add(BatchNormalization(momentum=0.8))\n",
        "model.add(Dense(400, activation='relu'))\n",
        "model.add(BatchNormalization(momentum=0.8))\n",
        "model.add(Dense(400, activation='relu'))\n",
        "model.add(BatchNormalization(momentum=0.8))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ov8EijXLwpk"
      },
      "source": [
        "# Compile the model\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VtbN-lMbLwpl"
      },
      "source": [
        "# Show the model summary\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6euItJ8bLwpn"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0-gu3b4Lwpo"
      },
      "source": [
        "# Create batched training and validation datasets\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSc4I6lhLwpp"
      },
      "source": [
        "# Shuffle the training data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BBzjSPpLLwpr"
      },
      "source": [
        "# Fit the model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKUL_6uvLwps"
      },
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}